{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f61c53a3-9b15-4252-9cbc-e625e81ba6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "device='cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4237935-2703-4d08-97af-7c57a275c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEnhancer(nn.Module):\n",
    "    def __init__(self, num_transformer_layers=2, num_heads=8, cnn_filters=[32, 64, 128, 256]):\n",
    "        super(AudioEnhancer, self).__init__()\n",
    "        \n",
    "        # CNN Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(2, cnn_filters[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(cnn_filters[0], cnn_filters[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(cnn_filters[1], cnn_filters[2], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(cnn_filters[2], cnn_filters[3], kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Transformer\n",
    "        self.transformer = TransformerEncoder(\n",
    "            TransformerEncoderLayer(d_model=cnn_filters[-1], nhead=num_heads, dim_feedforward=512, activation='relu'),\n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "        \n",
    "        # CNN Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(cnn_filters[3], cnn_filters[2], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(cnn_filters[2], cnn_filters[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(cnn_filters[1], cnn_filters[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(cnn_filters[0], 2, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input: [batch_size, 2, 48000]\n",
    "        \n",
    "        # CNN Encoder\n",
    "        x = self.encoder(x)  # Shape: [batch_size, cnn_filters[-1], 48000]\n",
    "        \n",
    "        # Permute for Transformer\n",
    "        x = x.permute(0, 2, 1)  # Shape: [batch_size, 48000, cnn_filters[-1]]\n",
    "        x = self.transformer(x)  # Shape: [batch_size, 48000, cnn_filters[-1]]\n",
    "        x = x.permute(0, 2, 1)  # Shape: [batch_size, cnn_filters[-1], 48000]\n",
    "        \n",
    "        # CNN Decoder\n",
    "        x = self.decoder(x)  # Shape: [batch_size, 2, 48000]\n",
    "        \n",
    "        return x\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Compute perceptual features\n",
    "        pred_features = self.feature_extractor(pred)\n",
    "        target_features = self.feature_extractor(target)\n",
    "        \n",
    "        # Perceptual loss\n",
    "        perceptual_loss = self.mse_loss(pred_features, target_features)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss = self.mse_loss(pred, target)\n",
    "        \n",
    "        return perceptual_loss + reconstruction_loss\n",
    "\n",
    "class DummyFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DummyFeatureExtractor, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(2, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = DummyFeatureExtractor().to(device)\n",
    "loss_fn = PerceptualLoss(feature_extractor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e96fb0ab-ac44-4e72-8105-697f3a0db9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2254906/466762637.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"/home/j597s263/Models/Audio.mod\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AudioEnhancer(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv1d(2, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (3): ReLU()\n",
       "    (4): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (5): ReLU()\n",
       "    (6): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (7): ReLU()\n",
       "  )\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (3): ReLU()\n",
       "    (4): ConvTranspose1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose1d(32, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (7): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device='cuda:1'\n",
    "# Load the entire model\n",
    "model = torch.load(\"/home/j597s263/Models/Audio.mod\")\n",
    "model.eval()  # Set to evaluation mode\n",
    "model.to(device)  # Move to the appropriate device (CPU or GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6f8f9a5-380b-4081-8093-d0adae8495bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced audio saved to: /home/j597s263/Datasets/Audio/Test/Enhanced.wav\n"
     ]
    }
   ],
   "source": [
    "def split_into_chunks(waveform, chunk_size):\n",
    "    \"\"\"\n",
    "    Splits the audio waveform into chunks of the specified size.\n",
    "    Args:\n",
    "        waveform: Tensor of shape [2, total_samples].\n",
    "        chunk_size: Size of each chunk (e.g., 48000 for 1 second).\n",
    "    Returns:\n",
    "        List of waveform chunks.\n",
    "    \"\"\"\n",
    "    num_samples = waveform.shape[1]\n",
    "    return [waveform[:, i:i + chunk_size] for i in range(0, num_samples, chunk_size) if waveform[:, i:i + chunk_size].shape[1] == chunk_size]\n",
    "\n",
    "def reconstruct_from_chunks(chunks):\n",
    "    \"\"\"\n",
    "    Reconstructs the full waveform from chunks by concatenating them.\n",
    "    Args:\n",
    "        chunks: List of tensors of shape [2, chunk_size].\n",
    "    Returns:\n",
    "        Tensor of shape [2, total_samples].\n",
    "    \"\"\"\n",
    "    return torch.cat(chunks, dim=1)\n",
    "\n",
    "# Example: Process Test Audio\n",
    "def process_audio_in_batches(model, waveform, sample_rate, chunk_size=48000):\n",
    "    \"\"\"\n",
    "    Processes the input audio in smaller chunks and reconstructs the full output.\n",
    "    Args:\n",
    "        model: The trained model.\n",
    "        waveform: Input waveform of shape [2, total_samples].\n",
    "        sample_rate: Sample rate of the audio.\n",
    "        chunk_size: Size of each chunk (default is 48000 for 1 second).\n",
    "    Returns:\n",
    "        Enhanced waveform of shape [2, total_samples].\n",
    "    \"\"\"\n",
    "    # Split into chunks\n",
    "    chunks = split_into_chunks(waveform, chunk_size)\n",
    "    enhanced_chunks = []\n",
    "\n",
    "    # Process each chunk\n",
    "    model.eval()  # Ensure model is in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for chunk in chunks:\n",
    "            chunk = chunk.unsqueeze(0).to(device)  # Add batch dimension\n",
    "            enhanced_chunk = model(chunk)  # Shape: [1, 2, chunk_size]\n",
    "            enhanced_chunks.append(enhanced_chunk.squeeze(0).cpu())  # Remove batch dimension\n",
    "\n",
    "    # Reconstruct full audio\n",
    "    enhanced_waveform = reconstruct_from_chunks(enhanced_chunks)\n",
    "    return enhanced_waveform\n",
    "\n",
    "# Load and preprocess the test audio\n",
    "test_file = '/home/j597s263/Datasets/Audio/Test/Lossy'\n",
    "waveform, sample_rate = torchaudio.load(test_file)\n",
    "\n",
    "# Ensure stereo and match sample rate\n",
    "if waveform.shape[0] != 2:\n",
    "    waveform = torch.stack([waveform, waveform])  # Duplicate channel if mono\n",
    "if sample_rate != 48000:\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=48000)\n",
    "    waveform = resampler(waveform)\n",
    "\n",
    "# Process the audio in batches\n",
    "waveform = waveform.to(device)\n",
    "enhanced_waveform = process_audio_in_batches(model, waveform, sample_rate)\n",
    "\n",
    "# Save the enhanced output\n",
    "output_file = \"/home/j597s263/Datasets/Audio/Test/Enhanced.wav\"\n",
    "torchaudio.save(output_file, enhanced_waveform, sample_rate=48000)\n",
    "print(f\"Enhanced audio saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65ebbf2-4ab3-4c05-9c0b-18689287955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testf = '/home/j597s263/Datasets/Audio/Test/Lossy'\n",
    "waveform, sample_rate = torchaudio.load(testf)\n",
    "# Ensure stereo and match sample rate\n",
    "if waveform.shape[0] != 2:\n",
    "    waveform = torch.stack([waveform, waveform])  # Duplicate channel if mono\n",
    "if sample_rate != 48000:\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=48000)\n",
    "    waveform = resampler(waveform)\n",
    "\n",
    "# Prepare for model input\n",
    "waveform = waveform.unsqueeze(0).to(device)  # Add batch dimension, shape: [1, 2, 48000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddff27b-fced-4b59-9d3d-6aa4309ae807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate enhanced audio\n",
    "with torch.no_grad():\n",
    "    enhanced_waveform = model(waveform)  # Shape: [1, 2, 48000]\n",
    "enhanced_waveform = enhanced_waveform.squeeze(0).cpu()  # Remove batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288948a9-32a9-4e15-8478-8ef35a73fc59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
