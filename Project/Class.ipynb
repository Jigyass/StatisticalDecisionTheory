{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4938fef2-6e5e-4242-84ad-cb325af22b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "899ff0a1-9374-4320-9fc1-f4bfa5e09828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, lossless_dir, lossy_dir):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by processing all audio files in the given directories.\n",
    "        Each observation is a pair of corresponding lossy and lossless stereo segments.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "\n",
    "        # Get sorted lists of lossless and lossy files\n",
    "        lossless_files = sorted(\n",
    "            [os.path.join(lossless_dir, f) for f in os.listdir(lossless_dir) if os.path.isfile(os.path.join(lossless_dir, f))]\n",
    "        )\n",
    "        lossy_files = sorted(\n",
    "            [os.path.join(lossy_dir, f) for f in os.listdir(lossy_dir) if os.path.isfile(os.path.join(lossy_dir, f))]\n",
    "        )\n",
    "\n",
    "        # Ensure equal number of files\n",
    "        assert len(lossless_files) == len(lossy_files), \"Mismatch in number of lossless and lossy files!\"\n",
    "\n",
    "        # Process files and create dataset\n",
    "        for idx, (lossless_file, lossy_file) in enumerate(zip(lossless_files, lossy_files)):\n",
    "            self.data.extend(self.process_pair(lossless_file, lossy_file))\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"Processed {idx + 1} files...\")\n",
    "\n",
    "        print(f\"Dataset created with {len(self.data)} segment pairs.\")\n",
    "\n",
    "    def process_pair(self, lossless_path, lossy_path):\n",
    "        \"\"\"\n",
    "        Processes a pair of lossless and lossy files into aligned stereo segments.\n",
    "        \"\"\"\n",
    "        # Preprocess lossless and lossy audio\n",
    "        lossless_segments, lossless_segment_size = self.preprocess(lossless_path)\n",
    "        lossy_segments, lossy_segment_size = self.preprocess(lossy_path)\n",
    "\n",
    "        # Handle floating-point mismatch in segment counts\n",
    "        if len(lossless_segments) > len(lossy_segments):\n",
    "            for _ in range(len(lossless_segments) - len(lossy_segments)):\n",
    "                # Add padded segments to lossy segments to match lossless count\n",
    "                padding_segment = torch.zeros_like(lossless_segments[0])\n",
    "                lossy_segments.append(padding_segment)\n",
    "\n",
    "        # Randomly pad lossy segments to match lossless size\n",
    "        padded_lossy_segments = [\n",
    "            self.random_pad(lossy_segment, lossless_segment.shape[1])\n",
    "            for lossy_segment, lossless_segment in zip(lossy_segments, lossless_segments)\n",
    "        ]\n",
    "\n",
    "        return list(zip(padded_lossy_segments, lossless_segments))\n",
    "\n",
    "    def preprocess(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads an audio file, calculates dynamic segment size, and splits into stereo segments.\n",
    "        \"\"\"\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "        # Calculate dynamic segment size\n",
    "        segment_size = int((sample_rate / 1000) * 2)\n",
    "\n",
    "        # Split waveform into fixed-size segments\n",
    "        segments = [\n",
    "            waveform[:, i:i + segment_size]\n",
    "            for i in range(0, waveform.shape[1], segment_size)\n",
    "            if waveform[:, i:i + segment_size].shape[1] == segment_size\n",
    "        ]\n",
    "        return segments, segment_size\n",
    "\n",
    "    def random_pad(self, segment, target_size):\n",
    "        \"\"\"\n",
    "        Randomly pads the input segment to match the target size.\n",
    "        Args:\n",
    "            segment: Input segment of shape [2, current_size].\n",
    "            target_size: Desired target size (number of samples).\n",
    "        Returns:\n",
    "            Padded segment of shape [2, target_size].\n",
    "        \"\"\"\n",
    "        current_size = segment.shape[1]\n",
    "        if current_size >= target_size:\n",
    "            return segment  # No padding needed\n",
    "\n",
    "        # Calculate padding size\n",
    "        padding_size = target_size - current_size\n",
    "        front_pad = random.randint(0, padding_size)\n",
    "        back_pad = padding_size - front_pad\n",
    "\n",
    "        # Apply padding\n",
    "        return torch.nn.functional.pad(segment, (front_pad, back_pad))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a single pair of lossy and lossless stereo segments.\n",
    "        \"\"\"\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c273015-f131-49b0-95ea-08875c1c3649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 files...\n",
      "Processed 20 files...\n",
      "Processed 30 files...\n",
      "Processed 40 files...\n",
      "Processed 50 files...\n",
      "Processed 60 files...\n",
      "Processed 70 files...\n",
      "Processed 80 files...\n",
      "Processed 90 files...\n",
      "Processed 100 files...\n"
     ]
    }
   ],
   "source": [
    "counter=0\n",
    "dataset = AudioDataset(lossless_dir='/home/j597s263/scratch/j597s263/Datasets/Audio/Lossless/', lossy_dir='/home/j597s263/scratch/j597s263/Datasets/Audio/Lossy/')\n",
    "\n",
    "print(f\"Total number of observations (segment pairs): {len(dataset)}\")\n",
    "\n",
    "# Example: Retrieve a single observation\n",
    "lossy_segment, lossless_segment = dataset[0]\n",
    "print(f\"Lossy segment shape: {lossy_segment.shape}\")\n",
    "print(f\"Lossless segment shape: {lossless_segment.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42100f35-5c6a-4303-8989-f593066ab47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.0019, -0.0102, -0.0161],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0071, -0.0132, -0.0156]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d694ecea-b150-4bc8-a186-994aab6feea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEnhancer(nn.Module):\n",
    "    def __init__(self, num_transformer_layers=2, num_heads=8, cnn_filters=[32, 64, 128, 256]):\n",
    "        super(AudioEnhancer, self).__init__()\n",
    "        \n",
    "        # CNN Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(2, cnn_filters[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(cnn_filters[0], cnn_filters[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(cnn_filters[1], cnn_filters[2], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(cnn_filters[2], cnn_filters[3], kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Transformer\n",
    "        self.transformer = TransformerEncoder(\n",
    "            TransformerEncoderLayer(d_model=cnn_filters[-1], nhead=num_heads, dim_feedforward=512, activation='relu'),\n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "        \n",
    "        # CNN Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(cnn_filters[3], cnn_filters[2], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(cnn_filters[2], cnn_filters[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(cnn_filters[1], cnn_filters[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(cnn_filters[0], 2, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input: [batch_size, 2, 48000]\n",
    "        \n",
    "        # CNN Encoder\n",
    "        x = self.encoder(x)  # Shape: [batch_size, cnn_filters[-1], 48000]\n",
    "        \n",
    "        # Permute for Transformer\n",
    "        x = x.permute(0, 2, 1)  # Shape: [batch_size, 48000, cnn_filters[-1]]\n",
    "        x = self.transformer(x)  # Shape: [batch_size, 48000, cnn_filters[-1]]\n",
    "        x = x.permute(0, 2, 1)  # Shape: [batch_size, cnn_filters[-1], 48000]\n",
    "        \n",
    "        # CNN Decoder\n",
    "        x = self.decoder(x)  # Shape: [batch_size, 2, 48000]\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d7b9c5-9a05-4673-b82a-01fc89adea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Compute perceptual features\n",
    "        pred_features = self.feature_extractor(pred)\n",
    "        target_features = self.feature_extractor(target)\n",
    "        \n",
    "        # Perceptual loss\n",
    "        perceptual_loss = self.mse_loss(pred_features, target_features)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss = self.mse_loss(pred, target)\n",
    "        \n",
    "        return perceptual_loss + reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b10a8850-e26d-4f76-ace3-69b4262a9981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, loss_fn, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for lossy, lossless in dataloader:\n",
    "            # Move to GPU if available\n",
    "            lossy, lossless = lossy.to(device), lossless.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(lossy)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(output, lossless)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataloader):.4f}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b130dd4-210e-4331-9049-e1296b05cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0312da15-f68a-45bb-a31c-fce80e2f7919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j597s263/StatisticalDecisionTheory/Audio/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:1'\n",
    "model = AudioEnhancer(num_transformer_layers=2, num_heads=8).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afb92f06-e88b-49b4-bbe0-0314da9db444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DummyFeatureExtractor, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(2, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = DummyFeatureExtractor().to(device)\n",
    "loss_fn = PerceptualLoss(feature_extractor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d87e25e6-776d-4d3c-b05c-ba0488ecd711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0048\n",
      "Epoch 2/10, Loss: 0.0015\n",
      "Epoch 3/10, Loss: 0.0014\n",
      "Epoch 4/10, Loss: 0.0013\n",
      "Epoch 5/10, Loss: 0.0013\n",
      "Epoch 6/10, Loss: 0.0012\n",
      "Epoch 7/10, Loss: 0.0012\n",
      "Epoch 8/10, Loss: 0.0012\n",
      "Epoch 9/10, Loss: 0.0012\n",
      "Epoch 10/10, Loss: 0.0012\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, audio, optimizer, loss_fn, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5224ab67-a321-48b3-abcd-690aca8026dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioEnhancer(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv1d(2, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (3): ReLU()\n",
       "    (4): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (5): ReLU()\n",
       "    (6): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (7): ReLU()\n",
       "  )\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (3): ReLU()\n",
       "    (4): ConvTranspose1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose1d(32, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (7): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cff9a77-9ed0-45e5-ae52-3dcebc96c8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/home/j597s263/Models/Audio.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee687101-cdbe-450c-97d4-45a0d870ddfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
