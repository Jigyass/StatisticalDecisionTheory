{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4938fef2-6e5e-4242-84ad-cb325af22b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "899ff0a1-9374-4320-9fc1-f4bfa5e09828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, lossless_dir, lossy_dir):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by processing all audio files in the given directories.\n",
    "        Each observation is a pair of corresponding lossy and lossless stereo segments.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "\n",
    "        # Get sorted lists of lossless and lossy files\n",
    "        lossless_files = sorted(\n",
    "            [os.path.join(lossless_dir, f) for f in os.listdir(lossless_dir) if os.path.isfile(os.path.join(lossless_dir, f))]\n",
    "        )\n",
    "        lossy_files = sorted(\n",
    "            [os.path.join(lossy_dir, f) for f in os.listdir(lossy_dir) if os.path.isfile(os.path.join(lossy_dir, f))]\n",
    "        )\n",
    "\n",
    "        # Ensure equal number of files\n",
    "        assert len(lossless_files) == len(lossy_files), \"Mismatch in number of lossless and lossy files!\"\n",
    "\n",
    "        # Process files and create dataset\n",
    "        for lossless_file, lossy_file in zip(lossless_files, lossy_files):\n",
    "            self.data.extend(self.process_pair(lossless_file, lossy_file))\n",
    "        \n",
    "        print(f\"Dataset created with {len(self.data)} segment pairs.\")\n",
    "\n",
    "    def process_pair(self, lossless_path, lossy_path):\n",
    "        \"\"\"\n",
    "        Processes a pair of lossless and lossy files into aligned stereo segments.\n",
    "        \"\"\"\n",
    "        # Preprocess lossless and lossy audio\n",
    "        lossless_segments, lossless_rate = self.preprocess(lossless_path)\n",
    "        lossy_segments, lossy_rate = self.preprocess(lossy_path)\n",
    "\n",
    "        # Calculate segment length dynamically\n",
    "        lossless_segment_length = int(lossless_rate / 2000)\n",
    "        lossy_segment_length = int(lossy_rate / 2000)\n",
    "\n",
    "        # Split into dynamic-length segments\n",
    "        lossless_segments = self.split_and_pad(lossless_segments, lossless_segment_length)\n",
    "        lossy_segments = self.split_and_pad(lossy_segments, lossy_segment_length)\n",
    "\n",
    "        # Ensure equal number of segments\n",
    "        assert len(lossless_segments) == len(lossy_segments), f\"Segment mismatch in {lossless_path} and {lossy_path}!\"\n",
    "        return list(zip(lossy_segments, lossless_segments))\n",
    "\n",
    "    def preprocess(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads an audio file and returns the waveform and sample rate.\n",
    "        \"\"\"\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        #print(f\"Loaded {file_path} with shape: {waveform.shape} and sample rate: {sample_rate}\")\n",
    "\n",
    "        # Ensure the sample rate is 48,000 Hz for consistency\n",
    "        if sample_rate != 48000:\n",
    "            resampler = T.Resample(orig_freq=sample_rate, new_freq=48000)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        return waveform, 48000\n",
    "\n",
    "    def split_and_pad(self, waveform, segment_length):\n",
    "        \"\"\"\n",
    "        Splits a waveform into segments of the specified length and pads them equally.\n",
    "        Args:\n",
    "            waveform: Tensor of shape [2, total_samples].\n",
    "            segment_length: Number of samples per segment.\n",
    "        Returns:\n",
    "            List of padded segments.\n",
    "        \"\"\"\n",
    "        total_samples = waveform.shape[1]\n",
    "\n",
    "        # Calculate padding to make total length a multiple of segment_length\n",
    "        pad_length = (segment_length - total_samples % segment_length) % segment_length\n",
    "\n",
    "        # Pad waveform equally on both ends\n",
    "        pad_start = pad_length // 2\n",
    "        pad_end = pad_length - pad_start\n",
    "        waveform = torch.nn.functional.pad(waveform, (pad_start, pad_end))\n",
    "\n",
    "        # Split into segments\n",
    "        segments = [\n",
    "            waveform[:, i:i + segment_length]\n",
    "            for i in range(0, waveform.shape[1], segment_length)\n",
    "        ]\n",
    "        return segments\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a single pair of lossy and lossless stereo segments.\n",
    "        \"\"\"\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c273015-f131-49b0-95ea-08875c1c3649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /home/j597s263/scratch/j597s263/Datasets/Audio/Lossless/1 with shape: torch.Size([2, 4823066]) and sample rate: 48000\n",
      "Loaded /home/j597s263/scratch/j597s263/Datasets/Audio/Lossy/1 with shape: torch.Size([2, 3215378]) and sample rate: 32000\n",
      "Loaded /home/j597s263/scratch/j597s263/Datasets/Audio/Lossless/10 with shape: torch.Size([2, 8436038]) and sample rate: 48000\n",
      "Loaded /home/j597s263/scratch/j597s263/Datasets/Audio/Lossy/10 with shape: torch.Size([2, 5624026]) and sample rate: 32000\n",
      "Loaded /home/j597s263/scratch/j597s263/Datasets/Audio/Lossless/100 with shape: torch.Size([2, 12068352]) and sample rate: 48000\n",
      "Loaded /home/j597s263/scratch/j597s263/Datasets/Audio/Lossy/100 with shape: torch.Size([2, 11087799]) and sample rate: 44100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mAudioDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlossless_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/j597s263/scratch/j597s263/Datasets/Audio/Lossless/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlossy_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/j597s263/scratch/j597s263/Datasets/Audio/Lossy/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal number of observations (segment pairs): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Example: Retrieve a single observation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m, in \u001b[0;36mAudioDataset.__init__\u001b[0;34m(self, lossless_dir, lossy_dir)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Process files and create dataset\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lossless_file, lossy_file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lossless_files, lossy_files):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlossless_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlossy_file\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset created with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m segment pairs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 40\u001b[0m, in \u001b[0;36mAudioDataset.process_pair\u001b[0;34m(self, lossless_path, lossy_path)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Split into dynamic-length segments\u001b[39;00m\n\u001b[1;32m     39\u001b[0m lossless_segments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_and_pad(lossless_segments, lossless_segment_length)\n\u001b[0;32m---> 40\u001b[0m lossy_segments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_and_pad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlossy_segments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlossy_segment_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Ensure equal number of segments\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lossless_segments) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(lossy_segments), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSegment mismatch in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlossless_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlossy_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[5], line 80\u001b[0m, in \u001b[0;36mAudioDataset.split_and_pad\u001b[0;34m(self, waveform, segment_length)\u001b[0m\n\u001b[1;32m     77\u001b[0m waveform \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(waveform, (pad_start, pad_end))\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Split into segments\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m segments \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     81\u001b[0m     waveform[:, i:i \u001b[38;5;241m+\u001b[39m segment_length]\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, waveform\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], segment_length)\n\u001b[1;32m     83\u001b[0m ]\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m segments\n",
      "Cell \u001b[0;32mIn[5], line 81\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m waveform \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(waveform, (pad_start, pad_end))\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Split into segments\u001b[39;00m\n\u001b[1;32m     80\u001b[0m segments \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 81\u001b[0m     \u001b[43mwaveform\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msegment_length\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, waveform\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], segment_length)\n\u001b[1;32m     83\u001b[0m ]\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m segments\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = AudioDataset(lossless_dir='/home/j597s263/scratch/j597s263/Datasets/Audio/Lossless/', lossy_dir='/home/j597s263/scratch/j597s263/Datasets/Audio/Lossy/')\n",
    "\n",
    "print(f\"Total number of observations (segment pairs): {len(dataset)}\")\n",
    "\n",
    "# Example: Retrieve a single observation\n",
    "lossy_segment, lossless_segment = dataset[0]\n",
    "print(f\"Lossy segment shape: {lossy_segment.shape}\")\n",
    "print(f\"Lossless segment shape: {lossless_segment.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42100f35-5c6a-4303-8989-f593066ab47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.0019, -0.0102, -0.0161],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0071, -0.0132, -0.0156]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d694ecea-b150-4bc8-a186-994aab6feea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEnhancer(nn.Module):\n",
    "    def __init__(self, num_transformer_layers=2, num_heads=8, cnn_filters=[32, 64, 128, 256]):\n",
    "        super(AudioEnhancer, self).__init__()\n",
    "        \n",
    "        # CNN Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(2, cnn_filters[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(cnn_filters[0], cnn_filters[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(cnn_filters[1], cnn_filters[2], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(cnn_filters[2], cnn_filters[3], kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Transformer\n",
    "        self.transformer = TransformerEncoder(\n",
    "            TransformerEncoderLayer(d_model=cnn_filters[-1], nhead=num_heads, dim_feedforward=512, activation='relu'),\n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "        \n",
    "        # CNN Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(cnn_filters[3], cnn_filters[2], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(cnn_filters[2], cnn_filters[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(cnn_filters[1], cnn_filters[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(cnn_filters[0], 2, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input: [batch_size, 2, 48000]\n",
    "        \n",
    "        # CNN Encoder\n",
    "        x = self.encoder(x)  # Shape: [batch_size, cnn_filters[-1], 48000]\n",
    "        \n",
    "        # Permute for Transformer\n",
    "        x = x.permute(0, 2, 1)  # Shape: [batch_size, 48000, cnn_filters[-1]]\n",
    "        x = self.transformer(x)  # Shape: [batch_size, 48000, cnn_filters[-1]]\n",
    "        x = x.permute(0, 2, 1)  # Shape: [batch_size, cnn_filters[-1], 48000]\n",
    "        \n",
    "        # CNN Decoder\n",
    "        x = self.decoder(x)  # Shape: [batch_size, 2, 48000]\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d7b9c5-9a05-4673-b82a-01fc89adea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Compute perceptual features\n",
    "        pred_features = self.feature_extractor(pred)\n",
    "        target_features = self.feature_extractor(target)\n",
    "        \n",
    "        # Perceptual loss\n",
    "        perceptual_loss = self.mse_loss(pred_features, target_features)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss = self.mse_loss(pred, target)\n",
    "        \n",
    "        return perceptual_loss + reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b10a8850-e26d-4f76-ace3-69b4262a9981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, loss_fn, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for lossy, lossless in dataloader:\n",
    "            # Move to GPU if available\n",
    "            lossy, lossless = lossy.to(device), lossless.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(lossy)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(output, lossless)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataloader):.4f}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b130dd4-210e-4331-9049-e1296b05cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0312da15-f68a-45bb-a31c-fce80e2f7919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j597s263/StatisticalDecisionTheory/Audio/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:1'\n",
    "model = AudioEnhancer(num_transformer_layers=2, num_heads=8).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afb92f06-e88b-49b4-bbe0-0314da9db444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DummyFeatureExtractor, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(2, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = DummyFeatureExtractor().to(device)\n",
    "loss_fn = PerceptualLoss(feature_extractor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d87e25e6-776d-4d3c-b05c-ba0488ecd711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0048\n",
      "Epoch 2/10, Loss: 0.0015\n",
      "Epoch 3/10, Loss: 0.0014\n",
      "Epoch 4/10, Loss: 0.0013\n",
      "Epoch 5/10, Loss: 0.0013\n",
      "Epoch 6/10, Loss: 0.0012\n",
      "Epoch 7/10, Loss: 0.0012\n",
      "Epoch 8/10, Loss: 0.0012\n",
      "Epoch 9/10, Loss: 0.0012\n",
      "Epoch 10/10, Loss: 0.0012\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, audio, optimizer, loss_fn, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5224ab67-a321-48b3-abcd-690aca8026dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioEnhancer(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv1d(2, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (3): ReLU()\n",
       "    (4): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (5): ReLU()\n",
       "    (6): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (7): ReLU()\n",
       "  )\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (3): ReLU()\n",
       "    (4): ConvTranspose1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose1d(32, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (7): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cff9a77-9ed0-45e5-ae52-3dcebc96c8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/home/j597s263/Models/Audio.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee687101-cdbe-450c-97d4-45a0d870ddfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
