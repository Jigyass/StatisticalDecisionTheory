{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d4ef7-3b5b-412c-81ea-fa41c146ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace63b3-91a2-4217-8c46-8f33c0c63692",
   "metadata": {},
   "outputs": [],
   "source": [
    "usps_train = datasets.USPS(root=\"./data\", train=True, transform=ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516d289-69cf-4056-a6dc-164500b647d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e5757f-ee0f-43f1-b3bd-78d80dd54e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indices = {i: [] for i in range(10)}\n",
    "for idx, (_, label) in enumerate(usps_train):\n",
    "    if len(class_indices[label]) < 500:\n",
    "        class_indices[label].append(idx)\n",
    "\n",
    "# Collect indices for the final dataset\n",
    "final_indices = []\n",
    "for indices in class_indices.values():\n",
    "    final_indices.extend(indices)\n",
    "\n",
    "# Subset the dataset\n",
    "train_dataset = Subset(usps_train, final_indices)\n",
    "\n",
    "# DataLoader\n",
    "minibatches = DataLoader(train_dataset, batch_size=500, shuffle=True)\n",
    "\n",
    "print(f\"Number of samples in training dataset: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f5162-4193-41e6-923f-fa68bbdcd3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=4, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=3, stride=2)\n",
    "        self.a = nn.Tanh()\n",
    "        self.s = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.a(self.conv1(X))\n",
    "        X = self.a(self.conv2(X))\n",
    "        return self.s(self.conv3(X))[:, :, 0, 0]\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=10, out_channels=16, kernel_size=3, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=3, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(8)\n",
    "        self.deconv3 = nn.ConvTranspose2d(in_channels=8, out_channels=1, kernel_size=4, stride=2)\n",
    "        self.a = nn.Tanh()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.a(self.bn1(self.deconv1(X)))\n",
    "        X = self.a(self.bn2(self.deconv2(X)))\n",
    "        X = self.a(self.deconv3(X))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4495719c-b875-48b6-99fc-697716c3fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizerG = torch.optim.SGD(G.parameters(), lr=0.15)\n",
    "optimizerD = torch.optim.SGD(D.parameters(), lr=0.15)\n",
    "\n",
    "epochs = 300\n",
    "CE_D = torch.zeros(epochs)\n",
    "CE_G = torch.zeros(epochs)\n",
    "\n",
    "CE_D = torch.zeros(epochs)\n",
    "CE_G = torch.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):  # Fixed 'epochs' variable conflict\n",
    "    for X, _ in minibatches:\n",
    "        # Loss accumulation for real images\n",
    "        D.zero_grad()\n",
    "        X_real = X.to(device)\n",
    "        Y_real = torch.ones(500).to(device)  # Match size of discriminator output\n",
    "        outD_real = D(X_real).squeeze()  # Ensure the output is [500]\n",
    "        loss_real = loss_fn(outD_real, Y_real)\n",
    "\n",
    "        # Loss accumulation for fake images\n",
    "        z = torch.randn(500, 10, 1, 1).to(device)\n",
    "        X_fake = G(z)\n",
    "        Y_fake = torch.zeros(500).to(device)  # Match size of discriminator output\n",
    "        outD_fake = D(X_fake).squeeze()  # Ensure the output is [500]\n",
    "        loss_fake = loss_fn(outD_fake, Y_fake)\n",
    "\n",
    "        # Gradient descent part for Discriminator\n",
    "        lossD = loss_real + loss_fake\n",
    "        lossD.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Training of the Generator\n",
    "        G.zero_grad()\n",
    "        z = torch.randn(500, 10, 1, 1).to(device)\n",
    "        Y = torch.ones(500).to(device)  # Generator tries to fool the discriminator\n",
    "        outG = G(z)\n",
    "        outD_fake_for_G = D(outG).squeeze()  # Ensure the output is [500]\n",
    "        lossG = loss_fn(outD_fake_for_G, Y)\n",
    "        lossG.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "    # Log the losses\n",
    "    CE_D[epoch] = lossD.item()\n",
    "    CE_G[epoch] = lossG.item()\n",
    "    print(f\"Epoch {epoch + 1}/5, Loss_D: {CE_D[epoch]:.4f}, Loss_G: {CE_G[epoch]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7ba695-4a21-41a7-a0b9-7b6bbda66daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(CE_G.numpy(), label=\"G\", color='blue')\n",
    "plt.plot(CE_D.numpy(), label=\"D\", color='red')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d924692-cb23-464f-b634-924ada2de9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.eval()\n",
    "z = torch.randn(20, 10, 1, 1, device=device)\n",
    "with torch.no_grad():\n",
    "    fake_images = G(z).cpu()\n",
    "\n",
    "# Display the generated images\n",
    "fig, axs = plt.subplots(4, 5, figsize=(8,8))\n",
    "axs = axs.flatten()\n",
    "for i in range(20):\n",
    "    axs[i].imshow(fake_images[i,0,:,:], cmap='gray')\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
